{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"Synanthropic/reading-analog-gauge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unoptimized for mac m1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from math import atan2, degrees\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GaugeReader:\n",
    "    def __init__(self):\n",
    "        self.input_shape = (224, 224, 3)\n",
    "        self.min_value = 0\n",
    "        self.max_value = 10\n",
    "        \n",
    "    def build_model(self):\n",
    "        # Modified model architecture to predict single value\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.input_shape),\n",
    "            tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1)  # Single output for gauge reading\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        # Load dataset from HuggingFace\n",
    "        print(\"Loading dataset...\")\n",
    "        dataset = load_dataset(\"Synanthropic/reading-analog-gauge\")\n",
    "        train_ds = dataset['train']\n",
    "        \n",
    "        def preprocess_single_example(example):\n",
    "            # Convert image to numpy array\n",
    "            image = np.array(example['image'])\n",
    "            \n",
    "            # Resize image\n",
    "            image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "            \n",
    "            # Normalize image\n",
    "            image = image.astype(np.float32) / 255.0\n",
    "            \n",
    "            # Get label\n",
    "            label = np.array(example['label'], dtype=np.float32)\n",
    "            \n",
    "            return {\n",
    "                'image': image,\n",
    "                'label': label\n",
    "            }\n",
    "        \n",
    "        print(\"Processing examples...\")\n",
    "        # Process all examples\n",
    "        processed_data = [preprocess_single_example(example) for example in train_ds]\n",
    "        \n",
    "        # Separate images and labels\n",
    "        images = np.array([example['image'] for example in processed_data])\n",
    "        labels = np.array([example['label'] for example in processed_data])\n",
    "        \n",
    "        print(f\"Dataset shape - Images: {images.shape}, Labels: {labels.shape}\")\n",
    "        \n",
    "        # Create TensorFlow dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "        \n",
    "        # Batch and shuffle\n",
    "        dataset = dataset.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    def train_model(self, epochs=10):\n",
    "        # Build model\n",
    "        model = self.build_model()\n",
    "        \n",
    "        print(\"Preparing dataset...\")\n",
    "        train_ds = self.prepare_dataset()\n",
    "        print(\"Dataset prepared successfully!\")\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Starting training...\")\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            epochs=epochs,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='loss',\n",
    "                    patience=3,\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='loss',\n",
    "                    factor=0.5,\n",
    "                    patience=2\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "    def convert_to_tflite(self, model):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        tflite_model = converter.convert()\n",
    "        return tflite_model\n",
    "\n",
    "    def save_model(self, tflite_model, model_path):\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(tflite_model, f)\n",
    "    \n",
    "    def predict_value(self, model, image_path):\n",
    "        # Load and preprocess image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Add batch dimension\n",
    "        image = np.expand_dims(image, 0)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = model.predict(image)\n",
    "        return float(prediction[0][0])\n",
    "\n",
    "    def visualize_prediction(self, image_path, prediction):\n",
    "        # Load and resize image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'Predicted Value: {prediction:.2f}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    # Initialize GaugeReader\n",
    "    gauge_reader = GaugeReader()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    model, history = gauge_reader.train_model(epochs=10)\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    print(\"Converting to TFLite...\")\n",
    "    tflite_model = gauge_reader.convert_to_tflite(model)\n",
    "    \n",
    "    # Save model\n",
    "    print(\"Saving model...\")\n",
    "    gauge_reader.save_model(tflite_model, 'gauge_reader_model.tflite')\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset structure\n",
    "dataset = load_dataset(\"Synanthropic/reading-analog-gauge\")\n",
    "train_ds = dataset['train']\n",
    "example = train_ds[0]\n",
    "print(\"Dataset keys:\", example.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized for mac m1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: training_checkpoints/epoch_04.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 18:28:27.455764: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-30 18:28:27.455783: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-12-30 18:28:27.455786: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-12-30 18:28:27.455821: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-30 18:28:27.455835: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from previous checkpoint...\n",
      "Converting to TFLite...\n",
      "Converting to TFLite format...\n",
      "INFO:tensorflow:Assets written to: /var/folders/rt/0x35vmdd32ldd69_8j6wrvgc0000gn/T/tmp1s0ea92r/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/rt/0x35vmdd32ldd69_8j6wrvgc0000gn/T/tmp1s0ea92r/assets\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from math import atan2, degrees\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  # Garbage collector\n",
    "import os\n",
    "\n",
    "class GaugeReader:\n",
    "    def __init__(self):\n",
    "        self.input_shape = (160, 160, 3)\n",
    "        self.min_value = 0\n",
    "        self.max_value = 10\n",
    "        self.batch_size = 16\n",
    "        self.checkpoint_dir = 'training_checkpoints'\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.input_shape),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        print(\"Loading dataset...\")\n",
    "        dataset = load_dataset(\n",
    "            \"Synanthropic/reading-analog-gauge\",\n",
    "            split='train'\n",
    "        )\n",
    "        \n",
    "        print(f\"Total examples in dataset: {len(dataset)}\")\n",
    "        \n",
    "        def preprocess_example(example):\n",
    "            # Load and resize image\n",
    "            image = np.array(example['image'])\n",
    "            image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "            image = image.astype(np.float32) / 255.0\n",
    "            label = float(example['label'])\n",
    "            return image, label\n",
    "\n",
    "        def generator():\n",
    "            for example in dataset:\n",
    "                image, label = preprocess_example(example)\n",
    "                yield image, label\n",
    "\n",
    "        tf_dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=self.input_shape, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        tf_dataset = tf_dataset.shuffle(1000)\n",
    "        tf_dataset = tf_dataset.batch(self.batch_size)\n",
    "        tf_dataset = tf_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        steps_per_epoch = len(dataset) // self.batch_size\n",
    "\n",
    "        return tf_dataset, steps_per_epoch\n",
    "\n",
    "    def train_model(self, epochs=10):\n",
    "        # Build model\n",
    "        model = self.build_model()\n",
    "        \n",
    "        print(\"Preparing dataset...\")\n",
    "        train_ds, steps_per_epoch = self.prepare_dataset()\n",
    "        print(f\"Dataset prepared successfully! Steps per epoch: {steps_per_epoch}\")\n",
    "        \n",
    "        # Define checkpoint paths with .keras extension\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, \"epoch_{epoch:02d}.keras\")\n",
    "        best_model_path = os.path.join(self.checkpoint_dir, \"best_model.keras\")\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Starting training...\")\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            callbacks=[\n",
    "                # Save after every epoch\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=checkpoint_path,\n",
    "                    save_weights_only=False,\n",
    "                    save_freq='epoch'\n",
    "                ),\n",
    "                # Save best model based on loss\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=best_model_path,\n",
    "                    save_best_only=True,\n",
    "                    monitor='loss',\n",
    "                    mode='min',\n",
    "                    save_weights_only=False\n",
    "                ),\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='loss',\n",
    "                    patience=3,\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='loss',\n",
    "                    factor=0.5,\n",
    "                    patience=2\n",
    "                ),\n",
    "                # Memory cleanup callback\n",
    "                tf.keras.callbacks.LambdaCallback(\n",
    "                    on_epoch_end=lambda epoch, logs: gc.collect()\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "    def load_latest_checkpoint(self):\n",
    "        \"\"\"Load the latest checkpoint if it exists.\"\"\"\n",
    "        checkpoints = [d for d in os.listdir(self.checkpoint_dir) \n",
    "                      if d.startswith('epoch_') and d.endswith('.keras')]\n",
    "        if not checkpoints:\n",
    "            return None\n",
    "            \n",
    "        latest_checkpoint = max(checkpoints)\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, latest_checkpoint)\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        return tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "    def convert_to_tflite(self, model):\n",
    "        print(\"Converting to TFLite format...\")\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        tflite_model = converter.convert()\n",
    "        return tflite_model\n",
    "\n",
    "    def save_model(self, tflite_model, model_path):\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(tflite_model, f)\n",
    "\n",
    "def main():\n",
    "    # Initialize GaugeReader\n",
    "    gauge_reader = GaugeReader()\n",
    "    \n",
    "    try:\n",
    "        # Check for existing checkpoints\n",
    "        existing_model = gauge_reader.load_latest_checkpoint()\n",
    "        if existing_model:\n",
    "            print(\"Resuming from previous checkpoint...\")\n",
    "            model = existing_model\n",
    "        else:\n",
    "            # Train model\n",
    "            print(\"Training new model...\")\n",
    "            model, history = gauge_reader.train_model(epochs=10)\n",
    "        \n",
    "        # Convert to TFLite\n",
    "        print(\"Converting to TFLite...\")\n",
    "        tflite_model = gauge_reader.convert_to_tflite(model)\n",
    "        \n",
    "        # Save model\n",
    "        print(\"Saving model...\")\n",
    "        gauge_reader.save_model(tflite_model, 'gauge_reader_model.tflite')\n",
    "        \n",
    "        # Plot training history if available\n",
    "        if 'history' in locals():\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history.history['loss'], label='Loss')\n",
    "            plt.title('Model Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history.history['mae'], label='MAE')\n",
    "            plt.title('Model MAE')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('MAE')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        # Clean up memory in case of error\n",
    "        gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15670774299620544029\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 13422472795634595793\n",
      "physical_device_desc: \"device: 0, name: METAL, pci bus id: <undefined>\"\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 18:30:26.618328: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-30 18:30:26.618445: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-12-30 18:30:26.618497: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-12-30 18:30:26.618562: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-30 18:30:26.618591: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimized for RTX 3060 gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faizal/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "I0000 00:00:1735302132.971772    6181 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9812 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "2024-12-27 17:52:28.489984: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:103] Profiler session initializing.\n",
      "2024-12-27 17:52:28.490012: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:118] Profiler session started.\n",
      "2024-12-27 17:52:28.490036: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1006] Profiler found 1 GPUs\n",
      "2024-12-27 17:52:28.518519: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:130] Profiler session tear down.\n",
      "2024-12-27 17:52:28.518593: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1213] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 17:52:40.442151: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 5 of 1000\n",
      "2024-12-27 17:52:50.781367: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 12 of 1000\n",
      "2024-12-27 17:53:10.919037: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 26 of 1000\n",
      "2024-12-27 17:53:21.097728: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 33 of 1000\n",
      "2024-12-27 17:53:40.070189: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 46 of 1000\n",
      "2024-12-27 17:53:50.271973: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 53 of 1000\n",
      "2024-12-27 17:54:10.366805: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 67 of 1000\n",
      "2024-12-27 17:54:20.535547: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 74 of 1000\n",
      "2024-12-27 17:54:30.954686: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 81 of 1000\n",
      "2024-12-27 17:54:49.909448: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 94 of 1000\n",
      "2024-12-27 17:54:59.961857: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 101 of 1000\n",
      "2024-12-27 17:55:10.522451: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 108 of 1000\n",
      "2024-12-27 17:55:20.930991: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 115 of 1000\n",
      "2024-12-27 17:55:40.111009: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 128 of 1000\n",
      "2024-12-27 17:56:00.705562: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 141 of 1000\n",
      "2024-12-27 17:56:20.980760: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 154 of 1000\n",
      "2024-12-27 17:56:40.447819: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 167 of 1000\n",
      "2024-12-27 17:56:50.757620: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 174 of 1000\n",
      "2024-12-27 17:57:11.022599: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 187 of 1000\n",
      "2024-12-27 17:57:30.250619: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 200 of 1000\n",
      "2024-12-27 17:57:40.481569: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 207 of 1000\n",
      "2024-12-27 17:57:59.751572: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 220 of 1000\n",
      "2024-12-27 17:58:10.319620: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 227 of 1000\n",
      "2024-12-27 17:58:20.659600: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 234 of 1000\n",
      "2024-12-27 17:58:30.675540: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 241 of 1000\n",
      "2024-12-27 17:58:49.964158: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 254 of 1000\n",
      "2024-12-27 17:59:00.728770: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 261 of 1000\n",
      "2024-12-27 17:59:21.256831: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 275 of 1000\n",
      "2024-12-27 17:59:39.847306: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 288 of 1000\n",
      "2024-12-27 17:59:49.937508: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 295 of 1000\n",
      "2024-12-27 18:00:00.588175: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 302 of 1000\n",
      "2024-12-27 18:00:20.947135: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 316 of 1000\n",
      "2024-12-27 18:00:40.292238: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 329 of 1000\n",
      "2024-12-27 18:00:50.657190: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 336 of 1000\n",
      "2024-12-27 18:01:00.875982: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 343 of 1000\n",
      "2024-12-27 18:01:20.759666: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 356 of 1000\n",
      "2024-12-27 18:01:30.777229: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 363 of 1000\n",
      "2024-12-27 18:01:50.062726: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 376 of 1000\n",
      "2024-12-27 18:02:00.237302: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 383 of 1000\n",
      "2024-12-27 18:02:10.334077: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 390 of 1000\n",
      "2024-12-27 18:02:29.967165: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 403 of 1000\n",
      "2024-12-27 18:02:40.934751: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 410 of 1000\n",
      "2024-12-27 18:02:51.178028: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 417 of 1000\n",
      "2024-12-27 18:03:10.011751: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 430 of 1000\n",
      "2024-12-27 18:03:20.350509: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 437 of 1000\n",
      "2024-12-27 18:03:30.897857: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 444 of 1000\n",
      "2024-12-27 18:03:41.044854: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 451 of 1000\n",
      "2024-12-27 18:04:00.727843: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 465 of 1000\n",
      "2024-12-27 18:04:10.892536: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 472 of 1000\n",
      "2024-12-27 18:04:30.350212: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 485 of 1000\n",
      "2024-12-27 18:04:40.666088: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 492 of 1000\n",
      "2024-12-27 18:05:01.084539: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 506 of 1000\n",
      "2024-12-27 18:05:19.734947: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 519 of 1000\n",
      "2024-12-27 18:05:30.040025: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 526 of 1000\n",
      "2024-12-27 18:05:40.435028: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 533 of 1000\n",
      "2024-12-27 18:06:01.048881: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 547 of 1000\n",
      "2024-12-27 18:06:19.872122: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 560 of 1000\n",
      "2024-12-27 18:06:30.611937: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 567 of 1000\n",
      "2024-12-27 18:06:50.350934: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 580 of 1000\n",
      "2024-12-27 18:07:00.615981: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 587 of 1000\n",
      "2024-12-27 18:07:10.995175: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 594 of 1000\n",
      "2024-12-27 18:07:29.980841: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 607 of 1000\n",
      "2024-12-27 18:07:40.558807: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 614 of 1000\n",
      "2024-12-27 18:07:50.793371: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 621 of 1000\n",
      "2024-12-27 18:08:09.740018: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 634 of 1000\n",
      "2024-12-27 18:08:19.923540: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 641 of 1000\n",
      "2024-12-27 18:08:30.094467: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 648 of 1000\n",
      "2024-12-27 18:08:40.552793: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 655 of 1000\n",
      "2024-12-27 18:08:50.892290: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 662 of 1000\n",
      "2024-12-27 18:09:10.936570: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 676 of 1000\n",
      "2024-12-27 18:09:21.399541: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 683 of 1000\n",
      "2024-12-27 18:09:40.021804: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 696 of 1000\n",
      "2024-12-27 18:09:50.310958: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 703 of 1000\n",
      "2024-12-27 18:10:10.682837: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 717 of 1000\n",
      "2024-12-27 18:10:20.722041: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 724 of 1000\n",
      "2024-12-27 18:10:39.845169: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 737 of 1000\n",
      "2024-12-27 18:10:50.452433: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 744 of 1000\n",
      "2024-12-27 18:11:00.967267: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 751 of 1000\n",
      "2024-12-27 18:11:20.667611: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 764 of 1000\n",
      "2024-12-27 18:11:40.166634: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 777 of 1000\n",
      "2024-12-27 18:11:50.974814: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 784 of 1000\n",
      "2024-12-27 18:12:09.911318: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 797 of 1000\n",
      "2024-12-27 18:12:20.736334: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 804 of 1000\n",
      "2024-12-27 18:12:39.896015: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 816 of 1000\n",
      "2024-12-27 18:12:51.126948: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 823 of 1000\n",
      "2024-12-27 18:13:10.804458: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 836 of 1000\n",
      "2024-12-27 18:13:29.947014: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 849 of 1000\n",
      "2024-12-27 18:13:40.291288: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 856 of 1000\n",
      "2024-12-27 18:13:51.185414: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 863 of 1000\n",
      "2024-12-27 18:14:10.829109: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 876 of 1000\n",
      "2024-12-27 18:14:20.997529: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 883 of 1000\n",
      "2024-12-27 18:14:39.741438: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 895 of 1000\n",
      "2024-12-27 18:14:49.912585: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 902 of 1000\n",
      "2024-12-27 18:15:00.342512: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 909 of 1000\n",
      "2024-12-27 18:15:10.474428: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 916 of 1000\n",
      "2024-12-27 18:15:30.697206: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 929 of 1000\n",
      "2024-12-27 18:15:50.660900: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 942 of 1000\n",
      "2024-12-27 18:16:00.976177: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 949 of 1000\n",
      "2024-12-27 18:16:20.566677: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 962 of 1000\n",
      "2024-12-27 18:16:30.887622: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 969 of 1000\n",
      "2024-12-27 18:16:50.319250: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 982 of 1000\n",
      "2024-12-27 18:17:00.674532: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 989 of 1000\n",
      "2024-12-27 18:17:11.027697: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 996 of 1000\n",
      "2024-12-27 18:17:16.816220: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1735303682.662298    7506 service.cc:148] XLA service 0x778d3c003460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1735303682.663560    7506 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-12-27 18:18:02.716982: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1735303682.862111    7506 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-12-27 18:18:03.886968: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1055', 68 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2024-12-27 18:18:03.994284: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1055', 60 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-12-27 18:18:04.048027: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1055', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown \u001b[1m1539s\u001b[0m 1539s/step - loss: 0.0012 - mae: 0.0280"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735303687.475291    7506 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     63/Unknown \u001b[1m4542s\u001b[0m 48s/step - loss: 0.0024 - mae: 0.0211"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 183\u001b[0m\n\u001b[1;32m    180\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 160\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    159\u001b[0m     gauge_reader \u001b[38;5;241m=\u001b[39m GaugeReader()\n\u001b[0;32m--> 160\u001b[0m     model, history \u001b[38;5;241m=\u001b[39m \u001b[43mgauge_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     tflite_model \u001b[38;5;241m=\u001b[39m gauge_reader\u001b[38;5;241m.\u001b[39mconvert_to_tflite(model)\n\u001b[1;32m    162\u001b[0m     gauge_reader\u001b[38;5;241m.\u001b[39msave_model(tflite_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgauge_reader_model.tflite\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 125\u001b[0m, in \u001b[0;36mGaugeReader.train_model\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Add TensorBoard callback for memory monitoring\u001b[39;00m\n\u001b[1;32m    120\u001b[0m tensorboard_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(\n\u001b[1;32m    121\u001b[0m     log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    122\u001b[0m     profile_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m500,520\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Profile a few batches\u001b[39;00m\n\u001b[1;32m    123\u001b[0m )\n\u001b[0;32m--> 125\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_callback\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from math import atan2, degrees\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "class GaugeReader:\n",
    "    def __init__(self):\n",
    "        self.input_shape = (160, 160, 3)  # Reduced input size\n",
    "        self.min_value = 0\n",
    "        self.max_value = 10\n",
    "        self.batch_size = 32  # Smaller batch size to reduce memory\n",
    "\n",
    "    def build_model(self):\n",
    "        # Lighter model architecture with fewer parameters\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.input_shape),\n",
    "            \n",
    "            # First conv block - reduced filters\n",
    "            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            # Second conv block\n",
    "            tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            # Third conv block\n",
    "            tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            # Fourth conv block\n",
    "            tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            \n",
    "            # Reduced dense layers\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        print(\"Loading dataset...\")\n",
    "        # Load dataset in streaming mode\n",
    "        dataset = load_dataset(\n",
    "            \"Synanthropic/reading-analog-gauge\",\n",
    "            streaming=True\n",
    "        )\n",
    "        train_ds = dataset['train']\n",
    "        \n",
    "        def preprocess_example(example):\n",
    "            image = np.array(example['image'])\n",
    "            image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "            image = (image.astype(np.float32) - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "            return image, example['label']\n",
    "        \n",
    "        # Create streaming datasets with generators\n",
    "        def generate_examples(split):\n",
    "            for example in split:\n",
    "                yield preprocess_example(example)\n",
    "        \n",
    "        # Calculate approximate sizes\n",
    "        total_size = 10000  # Approximate dataset size\n",
    "        val_size = int(0.1 * total_size)\n",
    "        \n",
    "        # Create datasets using from_generator\n",
    "        train_dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: generate_examples(train_ds.take(total_size - val_size)),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=self.input_shape, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        val_dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: generate_examples(train_ds.skip(total_size - val_size).take(val_size)),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=self.input_shape, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Optimize for performance while maintaining memory efficiency\n",
    "        train_dataset = (train_dataset\n",
    "            .shuffle(1000)  # Reduced buffer size\n",
    "            .batch(self.batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        \n",
    "        val_dataset = (val_dataset\n",
    "            .batch(self.batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def train_model(self, epochs=20):\n",
    "        # Configure memory growth\n",
    "        for device in tf.config.list_physical_devices('GPU'):\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "        \n",
    "        model = self.build_model()\n",
    "        train_ds, val_ds = self.prepare_dataset()\n",
    "        \n",
    "        # Add TensorBoard callback for memory monitoring\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir='./logs',\n",
    "            profile_batch='500,520'  # Profile a few batches\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=epochs,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    factor=0.5,\n",
    "                    patience=3,\n",
    "                    min_lr=1e-6\n",
    "                ),\n",
    "                tensorboard_callback\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "    def convert_to_tflite(self, model):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_types = [tf.float16]  # Use FP16 quantization\n",
    "        tflite_model = converter.convert()\n",
    "        return tflite_model\n",
    "\n",
    "    def save_model(self, tflite_model, model_path):\n",
    "        with open(model_path, 'wb') as f:\n",
    "            f.write(tflite_model)  # Save directly without pickle\n",
    "\n",
    "def main():\n",
    "    gauge_reader = GaugeReader()\n",
    "    model, history = gauge_reader.train_model(epochs=20)\n",
    "    tflite_model = gauge_reader.convert_to_tflite(model)\n",
    "    gauge_reader.save_model(tflite_model, 'gauge_reader_model.tflite')\n",
    "    \n",
    "    # Plot with reduced memory usage\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Val')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Train')\n",
    "    plt.plot(history.history['val_mae'], label='Val')\n",
    "    plt.title('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 17:51:51.975267: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735302112.075544    6181 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735302112.103413    6181 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-27 17:51:52.359926: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 19:12:05.432159: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:103] Profiler session initializing.\n",
      "2024-12-27 19:12:05.432183: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:118] Profiler session started.\n",
      "2024-12-27 19:12:05.437055: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:130] Profiler session tear down.\n",
      "2024-12-27 19:12:05.440574: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1213] CUPTI activity buffer flushed\n",
      "2024-12-27 19:12:17.525228: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 4 of 1000\n",
      "2024-12-27 19:12:27.841763: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 11 of 1000\n",
      "2024-12-27 19:12:48.472231: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 24 of 1000\n",
      "2024-12-27 19:13:08.577519: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 38 of 1000\n",
      "2024-12-27 19:13:18.655258: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 45 of 1000\n",
      "2024-12-27 19:13:38.265266: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 59 of 1000\n",
      "2024-12-27 19:13:58.126536: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 73 of 1000\n",
      "2024-12-27 19:14:17.911111: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 87 of 1000\n",
      "2024-12-27 19:14:37.813333: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 101 of 1000\n",
      "2024-12-27 19:14:58.165011: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 115 of 1000\n",
      "2024-12-27 19:15:17.585664: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 128 of 1000\n",
      "2024-12-27 19:15:27.624797: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 135 of 1000\n",
      "2024-12-27 19:15:37.769140: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 142 of 1000\n",
      "2024-12-27 19:15:48.099488: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 149 of 1000\n",
      "2024-12-27 19:15:58.307885: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 156 of 1000\n",
      "2024-12-27 19:16:18.515626: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 170 of 1000\n",
      "2024-12-27 19:16:38.022638: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 183 of 1000\n",
      "2024-12-27 19:16:57.427462: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 196 of 1000\n",
      "2024-12-27 19:17:07.483941: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 203 of 1000\n",
      "2024-12-27 19:17:18.074141: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 210 of 1000\n",
      "2024-12-27 19:17:28.167788: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 217 of 1000\n",
      "2024-12-27 19:17:38.330762: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 224 of 1000\n",
      "2024-12-27 19:17:58.577432: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 238 of 1000\n",
      "2024-12-27 19:18:08.685014: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 245 of 1000\n",
      "2024-12-27 19:18:27.825966: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 258 of 1000\n",
      "2024-12-27 19:18:47.999546: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 271 of 1000\n",
      "2024-12-27 19:18:58.318701: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 278 of 1000\n",
      "2024-12-27 19:19:08.627407: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 285 of 1000\n",
      "2024-12-27 19:19:27.812940: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 298 of 1000\n",
      "2024-12-27 19:19:47.320234: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 311 of 1000\n",
      "2024-12-27 19:19:58.156223: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 318 of 1000\n",
      "2024-12-27 19:20:08.334868: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 325 of 1000\n",
      "2024-12-27 19:20:27.581235: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 338 of 1000\n",
      "2024-12-27 19:20:38.231041: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 345 of 1000\n",
      "2024-12-27 19:20:48.386973: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 352 of 1000\n",
      "2024-12-27 19:20:58.492281: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 359 of 1000\n",
      "2024-12-27 19:21:17.829441: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 372 of 1000\n",
      "2024-12-27 19:21:27.955217: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 379 of 1000\n",
      "2024-12-27 19:21:38.532496: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 386 of 1000\n",
      "2024-12-27 19:21:58.030562: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 399 of 1000\n",
      "2024-12-27 19:22:18.140162: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 412 of 1000\n",
      "2024-12-27 19:22:38.289120: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 425 of 1000\n",
      "2024-12-27 19:22:57.654225: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 438 of 1000\n",
      "2024-12-27 19:23:08.026678: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 444 of 1000\n",
      "2024-12-27 19:23:18.506164: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 451 of 1000\n",
      "2024-12-27 19:23:37.964591: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 464 of 1000\n",
      "2024-12-27 19:23:48.197451: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 471 of 1000\n",
      "2024-12-27 19:23:58.842600: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 478 of 1000\n",
      "2024-12-27 19:24:18.554688: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 491 of 1000\n",
      "2024-12-27 19:24:38.296821: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 504 of 1000\n",
      "2024-12-27 19:24:57.425562: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 517 of 1000\n",
      "2024-12-27 19:25:17.440316: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 530 of 1000\n",
      "2024-12-27 19:25:27.685473: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 537 of 1000\n",
      "2024-12-27 19:25:38.479062: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 544 of 1000\n",
      "2024-12-27 19:25:57.702057: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 557 of 1000\n",
      "2024-12-27 19:26:18.170926: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 571 of 1000\n",
      "2024-12-27 19:26:28.587801: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 578 of 1000\n",
      "2024-12-27 19:26:48.783294: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 592 of 1000\n",
      "2024-12-27 19:27:07.805560: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 605 of 1000\n",
      "2024-12-27 19:27:18.037788: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 612 of 1000\n",
      "2024-12-27 19:27:37.916965: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 626 of 1000\n",
      "2024-12-27 19:27:49.007054: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 632 of 1000\n",
      "2024-12-27 19:28:08.965410: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 645 of 1000\n",
      "2024-12-27 19:28:27.898994: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 658 of 1000\n",
      "2024-12-27 19:28:38.312575: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 665 of 1000\n",
      "2024-12-27 19:28:57.558557: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 678 of 1000\n",
      "2024-12-27 19:29:08.306490: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 685 of 1000\n",
      "2024-12-27 19:29:18.398272: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 692 of 1000\n",
      "2024-12-27 19:29:28.398088: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 699 of 1000\n",
      "2024-12-27 19:29:48.127869: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 712 of 1000\n",
      "2024-12-27 19:29:58.195220: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 719 of 1000\n",
      "2024-12-27 19:30:17.374556: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 732 of 1000\n",
      "2024-12-27 19:30:28.014082: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 739 of 1000\n",
      "2024-12-27 19:30:48.248958: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 753 of 1000\n",
      "2024-12-27 19:31:07.950694: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 765 of 1000\n",
      "2024-12-27 19:31:18.272596: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 772 of 1000\n",
      "2024-12-27 19:31:37.361429: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 785 of 1000\n",
      "2024-12-27 19:31:47.679399: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 792 of 1000\n",
      "2024-12-27 19:31:58.535595: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 799 of 1000\n",
      "2024-12-27 19:32:08.715679: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 806 of 1000\n",
      "2024-12-27 19:32:27.853592: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 819 of 1000\n",
      "2024-12-27 19:32:38.288897: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 826 of 1000\n",
      "2024-12-27 19:32:57.624119: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 839 of 1000\n",
      "2024-12-27 19:33:18.111317: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 853 of 1000\n",
      "2024-12-27 19:33:28.317711: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 860 of 1000\n",
      "2024-12-27 19:33:38.470892: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 867 of 1000\n",
      "2024-12-27 19:33:57.826393: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 880 of 1000\n",
      "2024-12-27 19:34:08.527985: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 887 of 1000\n",
      "2024-12-27 19:34:27.536080: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 899 of 1000\n",
      "2024-12-27 19:34:37.964392: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 906 of 1000\n",
      "2024-12-27 19:34:48.326970: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 913 of 1000\n",
      "2024-12-27 19:35:07.535071: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 925 of 1000\n",
      "2024-12-27 19:35:17.865096: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 932 of 1000\n",
      "2024-12-27 19:35:28.530710: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 939 of 1000\n",
      "2024-12-27 19:35:47.798661: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 952 of 1000\n",
      "2024-12-27 19:35:58.569758: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 959 of 1000\n",
      "2024-12-27 19:36:17.657297: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 972 of 1000\n",
      "2024-12-27 19:36:27.786277: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 979 of 1000\n",
      "2024-12-27 19:36:38.204973: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 986 of 1000\n",
      "2024-12-27 19:36:48.307121: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 993 of 1000\n",
      "2024-12-27 19:36:58.184251: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11/Unknown \u001b[1m2565s\u001b[0m 97s/step - loss: 2.6546 - mae: 1.1359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 01e050d6-887c-4282-a4c1-2fb05ea4c2a4)')' thrown while requesting GET https://huggingface.co/datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7788333d4200>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 1a075cb6-9d14-46ac-9005-ab8914b3560c)')' thrown while requesting GET https://huggingface.co/datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7789274faae0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: da3da7a1-fc95-4a8f-83ef-b142c4919424)')' thrown while requesting GET https://huggingface.co/datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x77897b3f4b60>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 77dc11ab-5748-48d0-bfbf-ac903436d3cc)')' thrown while requesting GET https://huggingface.co/datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip\n",
      "Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     31/Unknown \u001b[1m4583s\u001b[0m 100s/step - loss: 1.6418 - mae: 0.8470"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "class GaugeReader:\n",
    "    def __init__(self):\n",
    "        self.input_shape = (160, 160, 3)\n",
    "        self.min_value = 0\n",
    "        self.max_value = 10\n",
    "        self.batch_size = 64  # Increased but still conservative for 12GB VRAM\n",
    "        \n",
    "        # Enable mixed precision for better memory efficiency\n",
    "        mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.input_shape),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        print(\"Loading dataset...\")\n",
    "        dataset = load_dataset(\n",
    "            \"Synanthropic/reading-analog-gauge\",\n",
    "            streaming=True\n",
    "        )\n",
    "        train_ds = dataset['train']\n",
    "        \n",
    "        @tf.function\n",
    "        def preprocess_image(image):\n",
    "            image = tf.image.resize(image, [self.input_shape[0], self.input_shape[1]])\n",
    "            image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "            return image\n",
    "\n",
    "        def generate_examples(split):\n",
    "            for example in split:\n",
    "                # Convert PIL image to numpy array\n",
    "                image = np.array(example['image'])\n",
    "                # Convert to tensor and preprocess\n",
    "                image = preprocess_image(image)\n",
    "                yield image, example['label']\n",
    "\n",
    "        # Calculate sizes\n",
    "        total_size = 10000\n",
    "        val_size = int(0.1 * total_size)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: generate_examples(train_ds.take(total_size - val_size)),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=self.input_shape, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        val_dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: generate_examples(train_ds.skip(total_size - val_size).take(val_size)),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=self.input_shape, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Optimize the input pipeline\n",
    "        train_dataset = (train_dataset\n",
    "            .cache()  # Cache after preprocessing\n",
    "            .shuffle(1000)\n",
    "            .batch(self.batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        \n",
    "        val_dataset = (val_dataset\n",
    "            .batch(self.batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def train_model(self, epochs=20):\n",
    "        # Configure GPU memory growth\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        model = self.build_model()\n",
    "        train_ds, val_ds = self.prepare_dataset()\n",
    "        \n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir='./logs',\n",
    "            profile_batch='100,120'\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=epochs,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    factor=0.5,\n",
    "                    patience=3,\n",
    "                    min_lr=1e-6\n",
    "                ),\n",
    "                tensorboard_callback\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "    def convert_to_tflite(self, model):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "        tflite_model = converter.convert()\n",
    "        return tflite_model\n",
    "\n",
    "    def save_model(self, tflite_model, model_path):\n",
    "        with open(model_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "\n",
    "def main():\n",
    "    gauge_reader = GaugeReader()\n",
    "    model, history = gauge_reader.train_model(epochs=20)\n",
    "    tflite_model = gauge_reader.convert_to_tflite(model)\n",
    "    gauge_reader.save_model(tflite_model, 'gauge_reader_model.tflite')\n",
    "    \n",
    "    # Plot with reduced memory usage\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Val')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Train')\n",
    "    plt.plot(history.history['val_mae'], label='Val')\n",
    "    plt.title('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
