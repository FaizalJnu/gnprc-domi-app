{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"Synanthropic/reading-analog-gauge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unoptimized for mac m1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from math import atan2, degrees\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GaugeReader:\n",
    "    def __init__(self):\n",
    "        self.input_shape = (224, 224, 3)\n",
    "        self.min_value = 0\n",
    "        self.max_value = 10\n",
    "        \n",
    "    def build_model(self):\n",
    "        # Modified model architecture to predict single value\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.input_shape),\n",
    "            tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1)  # Single output for gauge reading\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        # Load dataset from HuggingFace\n",
    "        print(\"Loading dataset...\")\n",
    "        dataset = load_dataset(\"Synanthropic/reading-analog-gauge\")\n",
    "        train_ds = dataset['train']\n",
    "        \n",
    "        def preprocess_single_example(example):\n",
    "            # Convert image to numpy array\n",
    "            image = np.array(example['image'])\n",
    "            \n",
    "            # Resize image\n",
    "            image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "            \n",
    "            # Normalize image\n",
    "            image = image.astype(np.float32) / 255.0\n",
    "            \n",
    "            # Get label\n",
    "            label = np.array(example['label'], dtype=np.float32)\n",
    "            \n",
    "            return {\n",
    "                'image': image,\n",
    "                'label': label\n",
    "            }\n",
    "        \n",
    "        print(\"Processing examples...\")\n",
    "        # Process all examples\n",
    "        processed_data = [preprocess_single_example(example) for example in train_ds]\n",
    "        \n",
    "        # Separate images and labels\n",
    "        images = np.array([example['image'] for example in processed_data])\n",
    "        labels = np.array([example['label'] for example in processed_data])\n",
    "        \n",
    "        print(f\"Dataset shape - Images: {images.shape}, Labels: {labels.shape}\")\n",
    "        \n",
    "        # Create TensorFlow dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "        \n",
    "        # Batch and shuffle\n",
    "        dataset = dataset.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    def train_model(self, epochs=10):\n",
    "        # Build model\n",
    "        model = self.build_model()\n",
    "        \n",
    "        print(\"Preparing dataset...\")\n",
    "        train_ds = self.prepare_dataset()\n",
    "        print(\"Dataset prepared successfully!\")\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Starting training...\")\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            epochs=epochs,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='loss',\n",
    "                    patience=3,\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='loss',\n",
    "                    factor=0.5,\n",
    "                    patience=2\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "    def convert_to_tflite(self, model):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        tflite_model = converter.convert()\n",
    "        return tflite_model\n",
    "\n",
    "    def save_model(self, tflite_model, model_path):\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(tflite_model, f)\n",
    "    \n",
    "    def predict_value(self, model, image_path):\n",
    "        # Load and preprocess image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Add batch dimension\n",
    "        image = np.expand_dims(image, 0)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = model.predict(image)\n",
    "        return float(prediction[0][0])\n",
    "\n",
    "    def visualize_prediction(self, image_path, prediction):\n",
    "        # Load and resize image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'Predicted Value: {prediction:.2f}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    # Initialize GaugeReader\n",
    "    gauge_reader = GaugeReader()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    model, history = gauge_reader.train_model(epochs=10)\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    print(\"Converting to TFLite...\")\n",
    "    tflite_model = gauge_reader.convert_to_tflite(model)\n",
    "    \n",
    "    # Save model\n",
    "    print(\"Saving model...\")\n",
    "    gauge_reader.save_model(tflite_model, 'gauge_reader_model.tflite')\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset structure\n",
    "dataset = load_dataset(\"Synanthropic/reading-analog-gauge\")\n",
    "train_ds = dataset['train']\n",
    "example = train_ds[0]\n",
    "print(\"Dataset keys:\", example.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized for mac m1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: training_checkpoints/epoch_04.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 18:28:27.455764: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-30 18:28:27.455783: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-12-30 18:28:27.455786: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-12-30 18:28:27.455821: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-30 18:28:27.455835: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from previous checkpoint...\n",
      "Converting to TFLite...\n",
      "Converting to TFLite format...\n",
      "INFO:tensorflow:Assets written to: /var/folders/rt/0x35vmdd32ldd69_8j6wrvgc0000gn/T/tmp1s0ea92r/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/rt/0x35vmdd32ldd69_8j6wrvgc0000gn/T/tmp1s0ea92r/assets\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from math import atan2, degrees\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  # Garbage collector\n",
    "import os\n",
    "\n",
    "class GaugeReader:\n",
    "    def __init__(self):\n",
    "        self.input_shape = (160, 160, 3)\n",
    "        self.min_value = 0\n",
    "        self.max_value = 10\n",
    "        self.batch_size = 16\n",
    "        self.checkpoint_dir = 'training_checkpoints'\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.input_shape),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        print(\"Loading dataset...\")\n",
    "        dataset = load_dataset(\n",
    "            \"Synanthropic/reading-analog-gauge\",\n",
    "            split='train'\n",
    "        )\n",
    "        \n",
    "        print(f\"Total examples in dataset: {len(dataset)}\")\n",
    "        \n",
    "        def preprocess_example(example):\n",
    "            # Load and resize image\n",
    "            image = np.array(example['image'])\n",
    "            image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "            image = image.astype(np.float32) / 255.0\n",
    "            label = float(example['label'])\n",
    "            return image, label\n",
    "\n",
    "        def generator():\n",
    "            for example in dataset:\n",
    "                image, label = preprocess_example(example)\n",
    "                yield image, label\n",
    "\n",
    "        tf_dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=self.input_shape, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        tf_dataset = tf_dataset.shuffle(1000)\n",
    "        tf_dataset = tf_dataset.batch(self.batch_size)\n",
    "        tf_dataset = tf_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        steps_per_epoch = len(dataset) // self.batch_size\n",
    "\n",
    "        return tf_dataset, steps_per_epoch\n",
    "\n",
    "    def train_model(self, epochs=10):\n",
    "        # Build model\n",
    "        model = self.build_model()\n",
    "        \n",
    "        print(\"Preparing dataset...\")\n",
    "        train_ds, steps_per_epoch = self.prepare_dataset()\n",
    "        print(f\"Dataset prepared successfully! Steps per epoch: {steps_per_epoch}\")\n",
    "        \n",
    "        # Define checkpoint paths with .keras extension\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, \"epoch_{epoch:02d}.keras\")\n",
    "        best_model_path = os.path.join(self.checkpoint_dir, \"best_model.keras\")\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Starting training...\")\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            callbacks=[\n",
    "                # Save after every epoch\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=checkpoint_path,\n",
    "                    save_weights_only=False,\n",
    "                    save_freq='epoch'\n",
    "                ),\n",
    "                # Save best model based on loss\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=best_model_path,\n",
    "                    save_best_only=True,\n",
    "                    monitor='loss',\n",
    "                    mode='min',\n",
    "                    save_weights_only=False\n",
    "                ),\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='loss',\n",
    "                    patience=3,\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='loss',\n",
    "                    factor=0.5,\n",
    "                    patience=2\n",
    "                ),\n",
    "                # Memory cleanup callback\n",
    "                tf.keras.callbacks.LambdaCallback(\n",
    "                    on_epoch_end=lambda epoch, logs: gc.collect()\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "    def load_latest_checkpoint(self):\n",
    "        \"\"\"Load the latest checkpoint if it exists.\"\"\"\n",
    "        checkpoints = [d for d in os.listdir(self.checkpoint_dir) \n",
    "                      if d.startswith('epoch_') and d.endswith('.keras')]\n",
    "        if not checkpoints:\n",
    "            return None\n",
    "            \n",
    "        latest_checkpoint = max(checkpoints)\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, latest_checkpoint)\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        return tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "    def convert_to_tflite(self, model):\n",
    "        print(\"Converting to TFLite format...\")\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        tflite_model = converter.convert()\n",
    "        return tflite_model\n",
    "\n",
    "    def save_model(self, tflite_model, model_path):\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(tflite_model, f)\n",
    "\n",
    "def main():\n",
    "    # Initialize GaugeReader\n",
    "    gauge_reader = GaugeReader()\n",
    "    \n",
    "    try:\n",
    "        # Check for existing checkpoints\n",
    "        existing_model = gauge_reader.load_latest_checkpoint()\n",
    "        if existing_model:\n",
    "            print(\"Resuming from previous checkpoint...\")\n",
    "            model = existing_model\n",
    "        else:\n",
    "            # Train model\n",
    "            print(\"Training new model...\")\n",
    "            model, history = gauge_reader.train_model(epochs=10)\n",
    "        \n",
    "        # Convert to TFLite\n",
    "        print(\"Converting to TFLite...\")\n",
    "        tflite_model = gauge_reader.convert_to_tflite(model)\n",
    "        \n",
    "        # Save model\n",
    "        print(\"Saving model...\")\n",
    "        gauge_reader.save_model(tflite_model, 'gauge_reader_model.tflite')\n",
    "        \n",
    "        # Plot training history if available\n",
    "        if 'history' in locals():\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history.history['loss'], label='Loss')\n",
    "            plt.title('Model Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history.history['mae'], label='MAE')\n",
    "            plt.title('Model MAE')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('MAE')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        # Clean up memory in case of error\n",
    "        gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15670774299620544029\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 13422472795634595793\n",
      "physical_device_desc: \"device: 0, name: METAL, pci bus id: <undefined>\"\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 18:30:26.618328: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-30 18:30:26.618445: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-12-30 18:30:26.618497: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-12-30 18:30:26.618562: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-30 18:30:26.618591: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimized for RTX 3060 gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from math import atan2, degrees\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GaugeReader:\n",
    "    def __init__(self):\n",
    "        # Increased input shape and batch size for better GPU utilization\n",
    "        self.input_shape = (224, 224, 3)  # Larger input size\n",
    "        self.min_value = 0\n",
    "        self.max_value = 10\n",
    "        self.batch_size = 64  # Larger batch size for GPU\n",
    "        \n",
    "    def build_model(self):\n",
    "        # Deeper model architecture utilizing more GPU memory\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.input_shape),\n",
    "            \n",
    "            # First conv block\n",
    "            tf.keras.layers.Conv2D(64, 3, activation=None, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(64, 3, activation=None, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            # Second conv block\n",
    "            tf.keras.layers.Conv2D(128, 3, activation=None, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(128, 3, activation=None, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            # Third conv block\n",
    "            tf.keras.layers.Conv2D(256, 3, activation=None, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(256, 3, activation=None, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            # Fourth conv block\n",
    "            tf.keras.layers.Conv2D(512, 3, activation=None, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(512, 3, activation=None, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            \n",
    "            # Dense layers\n",
    "            tf.keras.layers.Dense(1024, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        # Use mixed precision for faster training\n",
    "        mixed_precision.set_global_policy('mixed_float16')\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        print(\"Loading dataset...\")\n",
    "        dataset = load_dataset(\"Synanthropic/reading-analog-gauge\")\n",
    "        train_ds = dataset['train']\n",
    "        \n",
    "        # Calculate split sizes\n",
    "        total_size = len(train_ds)\n",
    "        val_size = int(0.1 * total_size)  # 10% validation split\n",
    "        train_size = total_size - val_size\n",
    "        \n",
    "        # Process all examples with parallel processing\n",
    "        def preprocess_example(example):\n",
    "            # Load and resize image\n",
    "            image = np.array(example['image'])\n",
    "            image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "            image = image.astype(np.float32) / 255.0\n",
    "            return image, example['label']\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            [preprocess_example(ex) for ex in train_ds[:train_size]]\n",
    "        )\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            [preprocess_example(ex) for ex in train_ds[train_size:]]\n",
    "        )\n",
    "        \n",
    "        # Optimize for GPU performance\n",
    "        train_dataset = (train_dataset\n",
    "            .shuffle(5000)\n",
    "            .batch(self.batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "            .cache()\n",
    "        )\n",
    "        \n",
    "        val_dataset = (val_dataset\n",
    "            .batch(self.batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "            .cache()\n",
    "        )\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def train_model(self, epochs=20):  # Increased epochs\n",
    "        # Set memory growth for GPU\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        if physical_devices:\n",
    "            for device in physical_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "        \n",
    "        # Build model\n",
    "        model = self.build_model()\n",
    "        \n",
    "        print(\"Preparing dataset...\")\n",
    "        train_ds, val_ds = self.prepare_dataset()\n",
    "        print(\"Dataset prepared successfully!\")\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Starting training...\")\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=epochs,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    factor=0.5,\n",
    "                    patience=3,\n",
    "                    min_lr=1e-6\n",
    "                ),\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    'best_model.h5',\n",
    "                    monitor='val_loss',\n",
    "                    save_best_only=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "    def convert_to_tflite(self, model):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        tflite_model = converter.convert()\n",
    "        return tflite_model\n",
    "\n",
    "    def save_model(self, tflite_model, model_path):\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(tflite_model, f)\n",
    "\n",
    "def main():\n",
    "    # Initialize GaugeReader\n",
    "    gauge_reader = GaugeReader()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    model, history = gauge_reader.train_model(epochs=20)\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    print(\"Converting to TFLite...\")\n",
    "    tflite_model = gauge_reader.convert_to_tflite(model)\n",
    "    \n",
    "    # Save model\n",
    "    print(\"Saving model...\")\n",
    "    gauge_reader.save_model(tflite_model, 'gauge_reader_model.tflite')\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_macos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
