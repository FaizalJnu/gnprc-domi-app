{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61541d90e8c64368b68db3c475ea7c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf8e21ee15441188fec66b6a1e0a4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "corners.zip:   0%|          | 0.00/1.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6597085f21ff45d39d7a8e1245c695da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "keypoint.zip:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20da2ad603a4296a77d04bc8b4d8ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/42442 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"Synanthropic/reading-analog-gauge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unoptimized for mac m1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Preparing dataset...\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing examples...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from math import atan2, degrees\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GaugeReader:\n",
    "    def __init__(self):\n",
    "        self.input_shape = (224, 224, 3)\n",
    "        self.min_value = 0\n",
    "        self.max_value = 10\n",
    "        \n",
    "    def build_model(self):\n",
    "        # Modified model architecture to predict single value\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.input_shape),\n",
    "            tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1)  # Single output for gauge reading\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        # Load dataset from HuggingFace\n",
    "        print(\"Loading dataset...\")\n",
    "        dataset = load_dataset(\"Synanthropic/reading-analog-gauge\")\n",
    "        train_ds = dataset['train']\n",
    "        \n",
    "        def preprocess_single_example(example):\n",
    "            # Convert image to numpy array\n",
    "            image = np.array(example['image'])\n",
    "            \n",
    "            # Resize image\n",
    "            image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "            \n",
    "            # Normalize image\n",
    "            image = image.astype(np.float32) / 255.0\n",
    "            \n",
    "            # Get label\n",
    "            label = np.array(example['label'], dtype=np.float32)\n",
    "            \n",
    "            return {\n",
    "                'image': image,\n",
    "                'label': label\n",
    "            }\n",
    "        \n",
    "        print(\"Processing examples...\")\n",
    "        # Process all examples\n",
    "        processed_data = [preprocess_single_example(example) for example in train_ds]\n",
    "        \n",
    "        # Separate images and labels\n",
    "        images = np.array([example['image'] for example in processed_data])\n",
    "        labels = np.array([example['label'] for example in processed_data])\n",
    "        \n",
    "        print(f\"Dataset shape - Images: {images.shape}, Labels: {labels.shape}\")\n",
    "        \n",
    "        # Create TensorFlow dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "        \n",
    "        # Batch and shuffle\n",
    "        dataset = dataset.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    def train_model(self, epochs=10):\n",
    "        # Build model\n",
    "        model = self.build_model()\n",
    "        \n",
    "        print(\"Preparing dataset...\")\n",
    "        train_ds = self.prepare_dataset()\n",
    "        print(\"Dataset prepared successfully!\")\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Starting training...\")\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            epochs=epochs,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='loss',\n",
    "                    patience=3,\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='loss',\n",
    "                    factor=0.5,\n",
    "                    patience=2\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "    def convert_to_tflite(self, model):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        tflite_model = converter.convert()\n",
    "        return tflite_model\n",
    "\n",
    "    def save_model(self, tflite_model, model_path):\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(tflite_model, f)\n",
    "    \n",
    "    def predict_value(self, model, image_path):\n",
    "        # Load and preprocess image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Add batch dimension\n",
    "        image = np.expand_dims(image, 0)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = model.predict(image)\n",
    "        return float(prediction[0][0])\n",
    "\n",
    "    def visualize_prediction(self, image_path, prediction):\n",
    "        # Load and resize image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'Predicted Value: {prediction:.2f}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    # Initialize GaugeReader\n",
    "    gauge_reader = GaugeReader()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    model, history = gauge_reader.train_model(epochs=10)\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    print(\"Converting to TFLite...\")\n",
    "    tflite_model = gauge_reader.convert_to_tflite(model)\n",
    "    \n",
    "    # Save model\n",
    "    print(\"Saving model...\")\n",
    "    gauge_reader.save_model(tflite_model, 'gauge_reader_model.tflite')\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset keys: dict_keys(['image', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# Check dataset structure\n",
    "dataset = load_dataset(\"Synanthropic/reading-analog-gauge\")\n",
    "train_ds = dataset['train']\n",
    "example = train_ds[0]\n",
    "print(\"Dataset keys:\", example.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized for mac m1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "No GPU found or GPU memory growth setting failed\n",
      "Preparing dataset...\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: The dataset length is unknown.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from math import atan2, degrees\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  # Garbage collector\n",
    "\n",
    "class GaugeReader:\n",
    "    def __init__(self):\n",
    "        self.input_shape = (160, 160, 3)\n",
    "        self.min_value = 0\n",
    "        self.max_value = 10\n",
    "        self.batch_size = 16\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.input_shape),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        print(\"Loading dataset...\")\n",
    "        dataset = load_dataset(\n",
    "            \"Synanthropic/reading-analog-gauge\",\n",
    "            split='train'\n",
    "        )\n",
    "        \n",
    "        print(f\"Total examples in dataset: {len(dataset)}\")\n",
    "        \n",
    "        def preprocess_example(example):\n",
    "            # Load and resize image\n",
    "            image = np.array(example['image'])\n",
    "            image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "            image = image.astype(np.float32) / 255.0\n",
    "            label = float(example['label'])\n",
    "            return image, label\n",
    "\n",
    "        # Create tf.data.Dataset directly from the dataset\n",
    "        def generator():\n",
    "            for example in dataset:\n",
    "                image, label = preprocess_example(example)\n",
    "                yield image, label\n",
    "\n",
    "        # Create dataset with the correct shapes and types\n",
    "        tf_dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=self.input_shape, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Shuffle, batch, and prefetch\n",
    "        tf_dataset = tf_dataset.shuffle(1000)\n",
    "        tf_dataset = tf_dataset.batch(self.batch_size)\n",
    "        tf_dataset = tf_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        # Calculate steps per epoch\n",
    "        steps_per_epoch = len(dataset) // self.batch_size\n",
    "\n",
    "        return tf_dataset, steps_per_epoch\n",
    "\n",
    "    def train_model(self, epochs=10):\n",
    "        # Build model\n",
    "        model = self.build_model()\n",
    "        \n",
    "        print(\"Preparing dataset...\")\n",
    "        train_ds, steps_per_epoch = self.prepare_dataset()\n",
    "        print(f\"Dataset prepared successfully! Steps per epoch: {steps_per_epoch}\")\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Starting training...\")\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='loss',\n",
    "                    patience=3,\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='loss',\n",
    "                    factor=0.5,\n",
    "                    patience=2\n",
    "                ),\n",
    "                # Memory cleanup callback\n",
    "                tf.keras.callbacks.LambdaCallback(\n",
    "                    on_epoch_end=lambda epoch, logs: gc.collect()\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "    def convert_to_tflite(self, model):\n",
    "        print(\"Converting to TFLite format...\")\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        tflite_model = converter.convert()\n",
    "        return tflite_model\n",
    "\n",
    "    def save_model(self, tflite_model, model_path):\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(tflite_model, f)\n",
    "\n",
    "def main():\n",
    "    # Initialize GaugeReader\n",
    "    gauge_reader = GaugeReader()\n",
    "    \n",
    "    try:\n",
    "        # Train model\n",
    "        print(\"Training model...\")\n",
    "        model, history = gauge_reader.train_model(epochs=10)\n",
    "        \n",
    "        # Convert to TFLite\n",
    "        print(\"Converting to TFLite...\")\n",
    "        tflite_model = gauge_reader.convert_to_tflite(model)\n",
    "        \n",
    "        # Save model\n",
    "        print(\"Saving model...\")\n",
    "        gauge_reader.save_model(tflite_model, 'gauge_reader_model.tflite')\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['mae'], label='MAE')\n",
    "        plt.title('Model MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        # Clean up memory in case of error\n",
    "        gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimized for RTX 3060 gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faizal/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "I0000 00:00:1735302132.971772    6181 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9812 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "2024-12-27 17:52:28.489984: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:103] Profiler session initializing.\n",
      "2024-12-27 17:52:28.490012: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:118] Profiler session started.\n",
      "2024-12-27 17:52:28.490036: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1006] Profiler found 1 GPUs\n",
      "2024-12-27 17:52:28.518519: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:130] Profiler session tear down.\n",
      "2024-12-27 17:52:28.518593: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1213] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 17:52:40.442151: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 5 of 1000\n",
      "2024-12-27 17:52:50.781367: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 12 of 1000\n",
      "2024-12-27 17:53:10.919037: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 26 of 1000\n",
      "2024-12-27 17:53:21.097728: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 33 of 1000\n",
      "2024-12-27 17:53:40.070189: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 46 of 1000\n",
      "2024-12-27 17:53:50.271973: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 53 of 1000\n",
      "2024-12-27 17:54:10.366805: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 67 of 1000\n",
      "2024-12-27 17:54:20.535547: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 74 of 1000\n",
      "2024-12-27 17:54:30.954686: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 81 of 1000\n",
      "2024-12-27 17:54:49.909448: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 94 of 1000\n",
      "2024-12-27 17:54:59.961857: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 101 of 1000\n",
      "2024-12-27 17:55:10.522451: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 108 of 1000\n",
      "2024-12-27 17:55:20.930991: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 115 of 1000\n",
      "2024-12-27 17:55:40.111009: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 128 of 1000\n",
      "2024-12-27 17:56:00.705562: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 141 of 1000\n",
      "2024-12-27 17:56:20.980760: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 154 of 1000\n",
      "2024-12-27 17:56:40.447819: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 167 of 1000\n",
      "2024-12-27 17:56:50.757620: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 174 of 1000\n",
      "2024-12-27 17:57:11.022599: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 187 of 1000\n",
      "2024-12-27 17:57:30.250619: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 200 of 1000\n",
      "2024-12-27 17:57:40.481569: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 207 of 1000\n",
      "2024-12-27 17:57:59.751572: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 220 of 1000\n",
      "2024-12-27 17:58:10.319620: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 227 of 1000\n",
      "2024-12-27 17:58:20.659600: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 234 of 1000\n",
      "2024-12-27 17:58:30.675540: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 241 of 1000\n",
      "2024-12-27 17:58:49.964158: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 254 of 1000\n",
      "2024-12-27 17:59:00.728770: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 261 of 1000\n",
      "2024-12-27 17:59:21.256831: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 275 of 1000\n",
      "2024-12-27 17:59:39.847306: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 288 of 1000\n",
      "2024-12-27 17:59:49.937508: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 295 of 1000\n",
      "2024-12-27 18:00:00.588175: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 302 of 1000\n",
      "2024-12-27 18:00:20.947135: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 316 of 1000\n",
      "2024-12-27 18:00:40.292238: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 329 of 1000\n",
      "2024-12-27 18:00:50.657190: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 336 of 1000\n",
      "2024-12-27 18:01:00.875982: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 343 of 1000\n",
      "2024-12-27 18:01:20.759666: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 356 of 1000\n",
      "2024-12-27 18:01:30.777229: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 363 of 1000\n",
      "2024-12-27 18:01:50.062726: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 376 of 1000\n",
      "2024-12-27 18:02:00.237302: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 383 of 1000\n",
      "2024-12-27 18:02:10.334077: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 390 of 1000\n",
      "2024-12-27 18:02:29.967165: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 403 of 1000\n",
      "2024-12-27 18:02:40.934751: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 410 of 1000\n",
      "2024-12-27 18:02:51.178028: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 417 of 1000\n",
      "2024-12-27 18:03:10.011751: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 430 of 1000\n",
      "2024-12-27 18:03:20.350509: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 437 of 1000\n",
      "2024-12-27 18:03:30.897857: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 444 of 1000\n",
      "2024-12-27 18:03:41.044854: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 451 of 1000\n",
      "2024-12-27 18:04:00.727843: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 465 of 1000\n",
      "2024-12-27 18:04:10.892536: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 472 of 1000\n",
      "2024-12-27 18:04:30.350212: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 485 of 1000\n",
      "2024-12-27 18:04:40.666088: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 492 of 1000\n",
      "2024-12-27 18:05:01.084539: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 506 of 1000\n",
      "2024-12-27 18:05:19.734947: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 519 of 1000\n",
      "2024-12-27 18:05:30.040025: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 526 of 1000\n",
      "2024-12-27 18:05:40.435028: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 533 of 1000\n",
      "2024-12-27 18:06:01.048881: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 547 of 1000\n",
      "2024-12-27 18:06:19.872122: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 560 of 1000\n",
      "2024-12-27 18:06:30.611937: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 567 of 1000\n",
      "2024-12-27 18:06:50.350934: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 580 of 1000\n",
      "2024-12-27 18:07:00.615981: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 587 of 1000\n",
      "2024-12-27 18:07:10.995175: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 594 of 1000\n",
      "2024-12-27 18:07:29.980841: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 607 of 1000\n",
      "2024-12-27 18:07:40.558807: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 614 of 1000\n",
      "2024-12-27 18:07:50.793371: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 621 of 1000\n",
      "2024-12-27 18:08:09.740018: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 634 of 1000\n",
      "2024-12-27 18:08:19.923540: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 641 of 1000\n",
      "2024-12-27 18:08:30.094467: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 648 of 1000\n",
      "2024-12-27 18:08:40.552793: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 655 of 1000\n",
      "2024-12-27 18:08:50.892290: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 662 of 1000\n",
      "2024-12-27 18:09:10.936570: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 676 of 1000\n",
      "2024-12-27 18:09:21.399541: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 683 of 1000\n",
      "2024-12-27 18:09:40.021804: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 696 of 1000\n",
      "2024-12-27 18:09:50.310958: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 703 of 1000\n",
      "2024-12-27 18:10:10.682837: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 717 of 1000\n",
      "2024-12-27 18:10:20.722041: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 724 of 1000\n",
      "2024-12-27 18:10:39.845169: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 737 of 1000\n",
      "2024-12-27 18:10:50.452433: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 744 of 1000\n",
      "2024-12-27 18:11:00.967267: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 751 of 1000\n",
      "2024-12-27 18:11:20.667611: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 764 of 1000\n",
      "2024-12-27 18:11:40.166634: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 777 of 1000\n",
      "2024-12-27 18:11:50.974814: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 784 of 1000\n",
      "2024-12-27 18:12:09.911318: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 797 of 1000\n",
      "2024-12-27 18:12:20.736334: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 804 of 1000\n",
      "2024-12-27 18:12:39.896015: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 816 of 1000\n",
      "2024-12-27 18:12:51.126948: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 823 of 1000\n",
      "2024-12-27 18:13:10.804458: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 836 of 1000\n",
      "2024-12-27 18:13:29.947014: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 849 of 1000\n",
      "2024-12-27 18:13:40.291288: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 856 of 1000\n",
      "2024-12-27 18:13:51.185414: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 863 of 1000\n",
      "2024-12-27 18:14:10.829109: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 876 of 1000\n",
      "2024-12-27 18:14:20.997529: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 883 of 1000\n",
      "2024-12-27 18:14:39.741438: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 895 of 1000\n",
      "2024-12-27 18:14:49.912585: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 902 of 1000\n",
      "2024-12-27 18:15:00.342512: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 909 of 1000\n",
      "2024-12-27 18:15:10.474428: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 916 of 1000\n",
      "2024-12-27 18:15:30.697206: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 929 of 1000\n",
      "2024-12-27 18:15:50.660900: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 942 of 1000\n",
      "2024-12-27 18:16:00.976177: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 949 of 1000\n",
      "2024-12-27 18:16:20.566677: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 962 of 1000\n",
      "2024-12-27 18:16:30.887622: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 969 of 1000\n",
      "2024-12-27 18:16:50.319250: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 982 of 1000\n",
      "2024-12-27 18:17:00.674532: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 989 of 1000\n",
      "2024-12-27 18:17:11.027697: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:4: Filling up shuffle buffer (this may take a while): 996 of 1000\n",
      "2024-12-27 18:17:16.816220: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1735303682.662298    7506 service.cc:148] XLA service 0x778d3c003460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1735303682.663560    7506 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-12-27 18:18:02.716982: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1735303682.862111    7506 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-12-27 18:18:03.886968: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1055', 68 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2024-12-27 18:18:03.994284: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1055', 60 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-12-27 18:18:04.048027: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1055', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown \u001b[1m1539s\u001b[0m 1539s/step - loss: 0.0012 - mae: 0.0280"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735303687.475291    7506 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     63/Unknown \u001b[1m4542s\u001b[0m 48s/step - loss: 0.0024 - mae: 0.0211"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 183\u001b[0m\n\u001b[1;32m    180\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 160\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    159\u001b[0m     gauge_reader \u001b[38;5;241m=\u001b[39m GaugeReader()\n\u001b[0;32m--> 160\u001b[0m     model, history \u001b[38;5;241m=\u001b[39m \u001b[43mgauge_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     tflite_model \u001b[38;5;241m=\u001b[39m gauge_reader\u001b[38;5;241m.\u001b[39mconvert_to_tflite(model)\n\u001b[1;32m    162\u001b[0m     gauge_reader\u001b[38;5;241m.\u001b[39msave_model(tflite_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgauge_reader_model.tflite\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 125\u001b[0m, in \u001b[0;36mGaugeReader.train_model\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Add TensorBoard callback for memory monitoring\u001b[39;00m\n\u001b[1;32m    120\u001b[0m tensorboard_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(\n\u001b[1;32m    121\u001b[0m     log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    122\u001b[0m     profile_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m500,520\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Profile a few batches\u001b[39;00m\n\u001b[1;32m    123\u001b[0m )\n\u001b[0;32m--> 125\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_callback\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/work/gnprc-domi-app/tf/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from math import atan2, degrees\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "class GaugeReader:\n",
    "    def __init__(self):\n",
    "        self.input_shape = (160, 160, 3)  # Reduced input size\n",
    "        self.min_value = 0\n",
    "        self.max_value = 10\n",
    "        self.batch_size = 32  # Smaller batch size to reduce memory\n",
    "\n",
    "    def build_model(self):\n",
    "        # Lighter model architecture with fewer parameters\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.input_shape),\n",
    "            \n",
    "            # First conv block - reduced filters\n",
    "            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            # Second conv block\n",
    "            tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            # Third conv block\n",
    "            tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            # Fourth conv block\n",
    "            tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            \n",
    "            # Reduced dense layers\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        print(\"Loading dataset...\")\n",
    "        # Load dataset in streaming mode\n",
    "        dataset = load_dataset(\n",
    "            \"Synanthropic/reading-analog-gauge\",\n",
    "            streaming=True\n",
    "        )\n",
    "        train_ds = dataset['train']\n",
    "        \n",
    "        def preprocess_example(example):\n",
    "            image = np.array(example['image'])\n",
    "            image = cv2.resize(image, (self.input_shape[0], self.input_shape[1]))\n",
    "            image = (image.astype(np.float32) - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "            return image, example['label']\n",
    "        \n",
    "        # Create streaming datasets with generators\n",
    "        def generate_examples(split):\n",
    "            for example in split:\n",
    "                yield preprocess_example(example)\n",
    "        \n",
    "        # Calculate approximate sizes\n",
    "        total_size = 10000  # Approximate dataset size\n",
    "        val_size = int(0.1 * total_size)\n",
    "        \n",
    "        # Create datasets using from_generator\n",
    "        train_dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: generate_examples(train_ds.take(total_size - val_size)),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=self.input_shape, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        val_dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: generate_examples(train_ds.skip(total_size - val_size).take(val_size)),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=self.input_shape, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Optimize for performance while maintaining memory efficiency\n",
    "        train_dataset = (train_dataset\n",
    "            .shuffle(1000)  # Reduced buffer size\n",
    "            .batch(self.batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        \n",
    "        val_dataset = (val_dataset\n",
    "            .batch(self.batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def train_model(self, epochs=20):\n",
    "        # Configure memory growth\n",
    "        for device in tf.config.list_physical_devices('GPU'):\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "        \n",
    "        model = self.build_model()\n",
    "        train_ds, val_ds = self.prepare_dataset()\n",
    "        \n",
    "        # Add TensorBoard callback for memory monitoring\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir='./logs',\n",
    "            profile_batch='500,520'  # Profile a few batches\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=epochs,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    factor=0.5,\n",
    "                    patience=3,\n",
    "                    min_lr=1e-6\n",
    "                ),\n",
    "                tensorboard_callback\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "    def convert_to_tflite(self, model):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_types = [tf.float16]  # Use FP16 quantization\n",
    "        tflite_model = converter.convert()\n",
    "        return tflite_model\n",
    "\n",
    "    def save_model(self, tflite_model, model_path):\n",
    "        with open(model_path, 'wb') as f:\n",
    "            f.write(tflite_model)  # Save directly without pickle\n",
    "\n",
    "def main():\n",
    "    gauge_reader = GaugeReader()\n",
    "    model, history = gauge_reader.train_model(epochs=20)\n",
    "    tflite_model = gauge_reader.convert_to_tflite(model)\n",
    "    gauge_reader.save_model(tflite_model, 'gauge_reader_model.tflite')\n",
    "    \n",
    "    # Plot with reduced memory usage\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Val')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Train')\n",
    "    plt.plot(history.history['val_mae'], label='Val')\n",
    "    plt.title('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 17:51:51.975267: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735302112.075544    6181 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735302112.103413    6181 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-27 17:51:52.359926: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 19:12:05.432159: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:103] Profiler session initializing.\n",
      "2024-12-27 19:12:05.432183: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:118] Profiler session started.\n",
      "2024-12-27 19:12:05.437055: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:130] Profiler session tear down.\n",
      "2024-12-27 19:12:05.440574: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1213] CUPTI activity buffer flushed\n",
      "2024-12-27 19:12:17.525228: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 4 of 1000\n",
      "2024-12-27 19:12:27.841763: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 11 of 1000\n",
      "2024-12-27 19:12:48.472231: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 24 of 1000\n",
      "2024-12-27 19:13:08.577519: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 38 of 1000\n",
      "2024-12-27 19:13:18.655258: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 45 of 1000\n",
      "2024-12-27 19:13:38.265266: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 59 of 1000\n",
      "2024-12-27 19:13:58.126536: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 73 of 1000\n",
      "2024-12-27 19:14:17.911111: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 87 of 1000\n",
      "2024-12-27 19:14:37.813333: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 101 of 1000\n",
      "2024-12-27 19:14:58.165011: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 115 of 1000\n",
      "2024-12-27 19:15:17.585664: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 128 of 1000\n",
      "2024-12-27 19:15:27.624797: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 135 of 1000\n",
      "2024-12-27 19:15:37.769140: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 142 of 1000\n",
      "2024-12-27 19:15:48.099488: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 149 of 1000\n",
      "2024-12-27 19:15:58.307885: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 156 of 1000\n",
      "2024-12-27 19:16:18.515626: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 170 of 1000\n",
      "2024-12-27 19:16:38.022638: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 183 of 1000\n",
      "2024-12-27 19:16:57.427462: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 196 of 1000\n",
      "2024-12-27 19:17:07.483941: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 203 of 1000\n",
      "2024-12-27 19:17:18.074141: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 210 of 1000\n",
      "2024-12-27 19:17:28.167788: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 217 of 1000\n",
      "2024-12-27 19:17:38.330762: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 224 of 1000\n",
      "2024-12-27 19:17:58.577432: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 238 of 1000\n",
      "2024-12-27 19:18:08.685014: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 245 of 1000\n",
      "2024-12-27 19:18:27.825966: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 258 of 1000\n",
      "2024-12-27 19:18:47.999546: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 271 of 1000\n",
      "2024-12-27 19:18:58.318701: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 278 of 1000\n",
      "2024-12-27 19:19:08.627407: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 285 of 1000\n",
      "2024-12-27 19:19:27.812940: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 298 of 1000\n",
      "2024-12-27 19:19:47.320234: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 311 of 1000\n",
      "2024-12-27 19:19:58.156223: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 318 of 1000\n",
      "2024-12-27 19:20:08.334868: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 325 of 1000\n",
      "2024-12-27 19:20:27.581235: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 338 of 1000\n",
      "2024-12-27 19:20:38.231041: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 345 of 1000\n",
      "2024-12-27 19:20:48.386973: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 352 of 1000\n",
      "2024-12-27 19:20:58.492281: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 359 of 1000\n",
      "2024-12-27 19:21:17.829441: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 372 of 1000\n",
      "2024-12-27 19:21:27.955217: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 379 of 1000\n",
      "2024-12-27 19:21:38.532496: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 386 of 1000\n",
      "2024-12-27 19:21:58.030562: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 399 of 1000\n",
      "2024-12-27 19:22:18.140162: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 412 of 1000\n",
      "2024-12-27 19:22:38.289120: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 425 of 1000\n",
      "2024-12-27 19:22:57.654225: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 438 of 1000\n",
      "2024-12-27 19:23:08.026678: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 444 of 1000\n",
      "2024-12-27 19:23:18.506164: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 451 of 1000\n",
      "2024-12-27 19:23:37.964591: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 464 of 1000\n",
      "2024-12-27 19:23:48.197451: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 471 of 1000\n",
      "2024-12-27 19:23:58.842600: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 478 of 1000\n",
      "2024-12-27 19:24:18.554688: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 491 of 1000\n",
      "2024-12-27 19:24:38.296821: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 504 of 1000\n",
      "2024-12-27 19:24:57.425562: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 517 of 1000\n",
      "2024-12-27 19:25:17.440316: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 530 of 1000\n",
      "2024-12-27 19:25:27.685473: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 537 of 1000\n",
      "2024-12-27 19:25:38.479062: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 544 of 1000\n",
      "2024-12-27 19:25:57.702057: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 557 of 1000\n",
      "2024-12-27 19:26:18.170926: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 571 of 1000\n",
      "2024-12-27 19:26:28.587801: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 578 of 1000\n",
      "2024-12-27 19:26:48.783294: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 592 of 1000\n",
      "2024-12-27 19:27:07.805560: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 605 of 1000\n",
      "2024-12-27 19:27:18.037788: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 612 of 1000\n",
      "2024-12-27 19:27:37.916965: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 626 of 1000\n",
      "2024-12-27 19:27:49.007054: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 632 of 1000\n",
      "2024-12-27 19:28:08.965410: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 645 of 1000\n",
      "2024-12-27 19:28:27.898994: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 658 of 1000\n",
      "2024-12-27 19:28:38.312575: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 665 of 1000\n",
      "2024-12-27 19:28:57.558557: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 678 of 1000\n",
      "2024-12-27 19:29:08.306490: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 685 of 1000\n",
      "2024-12-27 19:29:18.398272: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 692 of 1000\n",
      "2024-12-27 19:29:28.398088: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 699 of 1000\n",
      "2024-12-27 19:29:48.127869: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 712 of 1000\n",
      "2024-12-27 19:29:58.195220: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 719 of 1000\n",
      "2024-12-27 19:30:17.374556: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 732 of 1000\n",
      "2024-12-27 19:30:28.014082: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 739 of 1000\n",
      "2024-12-27 19:30:48.248958: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 753 of 1000\n",
      "2024-12-27 19:31:07.950694: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 765 of 1000\n",
      "2024-12-27 19:31:18.272596: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 772 of 1000\n",
      "2024-12-27 19:31:37.361429: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 785 of 1000\n",
      "2024-12-27 19:31:47.679399: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 792 of 1000\n",
      "2024-12-27 19:31:58.535595: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 799 of 1000\n",
      "2024-12-27 19:32:08.715679: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 806 of 1000\n",
      "2024-12-27 19:32:27.853592: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 819 of 1000\n",
      "2024-12-27 19:32:38.288897: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 826 of 1000\n",
      "2024-12-27 19:32:57.624119: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 839 of 1000\n",
      "2024-12-27 19:33:18.111317: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 853 of 1000\n",
      "2024-12-27 19:33:28.317711: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 860 of 1000\n",
      "2024-12-27 19:33:38.470892: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 867 of 1000\n",
      "2024-12-27 19:33:57.826393: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 880 of 1000\n",
      "2024-12-27 19:34:08.527985: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 887 of 1000\n",
      "2024-12-27 19:34:27.536080: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 899 of 1000\n",
      "2024-12-27 19:34:37.964392: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 906 of 1000\n",
      "2024-12-27 19:34:48.326970: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 913 of 1000\n",
      "2024-12-27 19:35:07.535071: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 925 of 1000\n",
      "2024-12-27 19:35:17.865096: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 932 of 1000\n",
      "2024-12-27 19:35:28.530710: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 939 of 1000\n",
      "2024-12-27 19:35:47.798661: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 952 of 1000\n",
      "2024-12-27 19:35:58.569758: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 959 of 1000\n",
      "2024-12-27 19:36:17.657297: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 972 of 1000\n",
      "2024-12-27 19:36:27.786277: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 979 of 1000\n",
      "2024-12-27 19:36:38.204973: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 986 of 1000\n",
      "2024-12-27 19:36:48.307121: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 993 of 1000\n",
      "2024-12-27 19:36:58.184251: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11/Unknown \u001b[1m2565s\u001b[0m 97s/step - loss: 2.6546 - mae: 1.1359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 01e050d6-887c-4282-a4c1-2fb05ea4c2a4)')' thrown while requesting GET https://huggingface.co/datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7788333d4200>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 1a075cb6-9d14-46ac-9005-ab8914b3560c)')' thrown while requesting GET https://huggingface.co/datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7789274faae0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: da3da7a1-fc95-4a8f-83ef-b142c4919424)')' thrown while requesting GET https://huggingface.co/datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x77897b3f4b60>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 77dc11ab-5748-48d0-bfbf-ac903436d3cc)')' thrown while requesting GET https://huggingface.co/datasets/Synanthropic/reading-analog-gauge/resolve/f9b31b44c9a693eddcc9c93a247ad30c3005ea07/corners.zip\n",
      "Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     31/Unknown \u001b[1m4583s\u001b[0m 100s/step - loss: 1.6418 - mae: 0.8470"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "class GaugeReader:\n",
    "    def __init__(self):\n",
    "        self.input_shape = (160, 160, 3)\n",
    "        self.min_value = 0\n",
    "        self.max_value = 10\n",
    "        self.batch_size = 64  # Increased but still conservative for 12GB VRAM\n",
    "        \n",
    "        # Enable mixed precision for better memory efficiency\n",
    "        mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=self.input_shape),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            \n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            \n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        print(\"Loading dataset...\")\n",
    "        dataset = load_dataset(\n",
    "            \"Synanthropic/reading-analog-gauge\",\n",
    "            streaming=True\n",
    "        )\n",
    "        train_ds = dataset['train']\n",
    "        \n",
    "        @tf.function\n",
    "        def preprocess_image(image):\n",
    "            image = tf.image.resize(image, [self.input_shape[0], self.input_shape[1]])\n",
    "            image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "            return image\n",
    "\n",
    "        def generate_examples(split):\n",
    "            for example in split:\n",
    "                # Convert PIL image to numpy array\n",
    "                image = np.array(example['image'])\n",
    "                # Convert to tensor and preprocess\n",
    "                image = preprocess_image(image)\n",
    "                yield image, example['label']\n",
    "\n",
    "        # Calculate sizes\n",
    "        total_size = 10000\n",
    "        val_size = int(0.1 * total_size)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: generate_examples(train_ds.take(total_size - val_size)),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=self.input_shape, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        val_dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: generate_examples(train_ds.skip(total_size - val_size).take(val_size)),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=self.input_shape, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Optimize the input pipeline\n",
    "        train_dataset = (train_dataset\n",
    "            .cache()  # Cache after preprocessing\n",
    "            .shuffle(1000)\n",
    "            .batch(self.batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        \n",
    "        val_dataset = (val_dataset\n",
    "            .batch(self.batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def train_model(self, epochs=20):\n",
    "        # Configure GPU memory growth\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        model = self.build_model()\n",
    "        train_ds, val_ds = self.prepare_dataset()\n",
    "        \n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir='./logs',\n",
    "            profile_batch='100,120'\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=epochs,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    factor=0.5,\n",
    "                    patience=3,\n",
    "                    min_lr=1e-6\n",
    "                ),\n",
    "                tensorboard_callback\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "    def convert_to_tflite(self, model):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "        tflite_model = converter.convert()\n",
    "        return tflite_model\n",
    "\n",
    "    def save_model(self, tflite_model, model_path):\n",
    "        with open(model_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "\n",
    "def main():\n",
    "    gauge_reader = GaugeReader()\n",
    "    model, history = gauge_reader.train_model(epochs=20)\n",
    "    tflite_model = gauge_reader.convert_to_tflite(model)\n",
    "    gauge_reader.save_model(tflite_model, 'gauge_reader_model.tflite')\n",
    "    \n",
    "    # Plot with reduced memory usage\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Val')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Train')\n",
    "    plt.plot(history.history['val_mae'], label='Val')\n",
    "    plt.title('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'rai', 'examples'}\n",
      "WARNING:absl:Found the following 4 warning(s) during the validation:\n",
      "  -  [Metadata(reading-analog-gauge)] Property \"http://mlcommons.org/croissant/citeAs\" is recommended, but does not exist.\n",
      "  -  [Metadata(reading-analog-gauge)] Property \"https://schema.org/datePublished\" is recommended, but does not exist.\n",
      "  -  [Metadata(reading-analog-gauge)] Property \"https://schema.org/license\" is recommended, but does not exist.\n",
      "  -  [Metadata(reading-analog-gauge)] Property \"https://schema.org/version\" is recommended, but does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tables: None\n",
      "\n",
      "Dataset metadata:\n",
      "Metadata(uuid=\"_:N47895c671c5e49338a32181b5b015fb5\")\n",
      "\n",
      "Error accessing records directly: An error occured during the sequential generation of the dataset, more specifically during the operation Download(repo)\n",
      "\n",
      "Schema information:\n",
      "No schema available\n",
      "\n",
      "Available fields:\n"
     ]
    }
   ],
   "source": [
    "from mlcroissant import Dataset\n",
    "ds = Dataset(jsonld=\"https://huggingface.co/api/datasets/Synanthropic/reading-analog-gauge/croissant\")\n",
    "records = ds.records(\"default\")\n",
    "\n",
    "# Print available record names/tables\n",
    "print(\"Available tables:\", ds.metadata)\n",
    "\n",
    "# Print dataset metadata\n",
    "print(\"\\nDataset metadata:\")\n",
    "print(ds.metadata)\n",
    "\n",
    "# Try to access records properly\n",
    "try:\n",
    "    # Different ways to try accessing the records\n",
    "    for record in records:\n",
    "        print(\"\\nFirst record structure:\", record)\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(f\"\\nError accessing records directly: {e}\")\n",
    "\n",
    "# Try to get schema information\n",
    "print(\"\\nSchema information:\")\n",
    "print(records.schema if hasattr(records, 'schema') else \"No schema available\")\n",
    "\n",
    "# Print any available fields or columns\n",
    "print(\"\\nAvailable fields:\")\n",
    "for field in records.fields if hasattr(records, 'fields') else []:\n",
    "    print(f\"- {field}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latest model on the roboflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "class GaugeReaderTF:\n",
    "    def __init__(self, label_map_path):\n",
    "        self.label_map = label_map_util.load_labelmap(label_map_path)\n",
    "        self.categories = label_map_util.convert_label_map_to_categories(\n",
    "            self.label_map, max_num_classes=3, use_display_name=True)\n",
    "        self.category_index = label_map_util.create_category_index(self.categories)\n",
    "        \n",
    "    def parse_tfrecord_fn(self, example_proto):\n",
    "        \"\"\"Parse TFRecord file format\"\"\"\n",
    "        feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
    "            'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "            'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
    "            'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "            'image/object/class/label': tf.io.VarLenFeature(tf.int64)\n",
    "        }\n",
    "        \n",
    "        example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "        # Get the image\n",
    "        image = tf.io.decode_raw(example['image/encoded'], tf.uint8)\n",
    "        height = tf.cast(example['image/height'], tf.int32)\n",
    "        width = tf.cast(example['image/width'], tf.int32)\n",
    "        image = tf.reshape(image, [height, width, 3])\n",
    "        \n",
    "        # Get the labels\n",
    "        labels = tf.cast(example['image/object/class/label'].values, tf.int32)\n",
    "        \n",
    "        # Get the bounding boxes\n",
    "        xmins = example['image/object/bbox/xmin'].values\n",
    "        xmaxs = example['image/object/bbox/xmax'].values\n",
    "        ymins = example['image/object/bbox/ymin'].values\n",
    "        ymaxs = example['image/object/bbox/ymax'].values\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'labels': labels,\n",
    "            'bboxes': tf.stack([xmins, ymins, xmaxs, ymaxs], axis=1)\n",
    "        }\n",
    "\n",
    "    def create_dataset(self, tfrecord_path, batch_size=8):\n",
    "        \"\"\"Create a dataset from TFRecord file\"\"\"\n",
    "        dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "        dataset = dataset.map(self.parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build and return a pre-trained object detection model\"\"\"\n",
    "        # Using SSD ResNet50 V1 FPN from TensorFlow Model Garden\n",
    "        pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "        detection_model = model_builder.build(\n",
    "            model_config=config_util.get_configs_from_pipeline_file(pipeline_config)['model'],\n",
    "            is_training=True\n",
    "        )\n",
    "        return detection_model\n",
    "\n",
    "    def calculate_angle(self, center, needle_tip):\n",
    "        \"\"\"Calculate angle between vertical line and needle\"\"\"\n",
    "        dx = needle_tip[0] - center[0]\n",
    "        dy = needle_tip[1] - center[1]\n",
    "        angle = math.degrees(math.atan2(dy, dx))\n",
    "        if angle < 0:\n",
    "            angle += 360\n",
    "        return angle\n",
    "\n",
    "    def get_reading_from_angle(self, angle, min_angle=45, max_angle=315, min_value=0, max_value=100):\n",
    "        \"\"\"Convert angle to gauge reading\"\"\"\n",
    "        # Normalize the angle to the gauge's range\n",
    "        if angle > max_angle:\n",
    "            angle -= 360\n",
    "            \n",
    "        # Calculate reading using linear interpolation\n",
    "        angle_range = max_angle - min_angle\n",
    "        value_range = max_value - min_value\n",
    "        \n",
    "        reading = ((angle - min_angle) / angle_range) * value_range + min_value\n",
    "        return round(reading, 1)\n",
    "\n",
    "    def process_predictions(self, predictions, image):\n",
    "        \"\"\"Process model predictions to get gauge reading\"\"\"\n",
    "        height, width = image.shape[:2]\n",
    "        boxes = predictions['detection_boxes'][0].numpy()\n",
    "        scores = predictions['detection_scores'][0].numpy()\n",
    "        classes = predictions['detection_classes'][0].numpy().astype(int)\n",
    "        \n",
    "        # Initialize variables to store detection results\n",
    "        center = None\n",
    "        needle_tip = None\n",
    "        gauge_box = None\n",
    "        \n",
    "        # Process each detection with score > 0.5\n",
    "        for box, score, class_id in zip(boxes, scores, classes):\n",
    "            if score > 0.5:\n",
    "                ymin, xmin, ymax, xmax = box\n",
    "                # Convert normalized coordinates to pixel coordinates\n",
    "                xmin = int(xmin * width)\n",
    "                xmax = int(xmax * width)\n",
    "                ymin = int(ymin * height)\n",
    "                ymax = int(ymax * height)\n",
    "                \n",
    "                if class_id == 1:  # Center\n",
    "                    center = (int((xmin + xmax) / 2), int((ymin + ymax) / 2))\n",
    "                elif class_id == 2:  # Gauge\n",
    "                    gauge_box = (xmin, ymin, xmax, ymax)\n",
    "                elif class_id == 3:  # Needle\n",
    "                    needle_tip = (int((xmin + xmax) / 2), int((ymin + ymax) / 2))\n",
    "        \n",
    "        if center and needle_tip:\n",
    "            angle = self.calculate_angle(center, needle_tip)\n",
    "            reading = self.get_reading_from_angle(angle)\n",
    "            \n",
    "            return {\n",
    "                'center': center,\n",
    "                'needle_tip': needle_tip,\n",
    "                'gauge_box': gauge_box,\n",
    "                'angle': angle,\n",
    "                'reading': reading\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    def visualize_results(self, image, results):\n",
    "        \"\"\"Visualize detection results on the image\"\"\"\n",
    "        output = image.copy()\n",
    "        \n",
    "        if results['gauge_box']:\n",
    "            x1, y1, x2, y2 = results['gauge_box']\n",
    "            cv2.rectangle(output, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        if results['center']:\n",
    "            cv2.circle(output, results['center'], 5, (0, 0, 255), -1)\n",
    "            \n",
    "        if results['needle_tip']:\n",
    "            cv2.circle(output, results['needle_tip'], 5, (255, 0, 0), -1)\n",
    "            cv2.line(output, results['center'], results['needle_tip'], (255, 0, 0), 2)\n",
    "            \n",
    "        # Add text with reading\n",
    "        cv2.putText(output, f\"Reading: {results['reading']}\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the gauge reader\n",
    "gauge_reader = GaugeReaderTF('path/to/Gauge_label_map.pbtxt')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = gauge_reader.create_dataset('../datasets/Analog Gauge Meter.v13-v06-crop.tfrecord/train/Guage.tfrecord')\n",
    "valid_dataset = gauge_reader.create_dataset('../datasets/Analog Gauge Meter.v13-v06-crop.tfrecord/valid/Gauge.tfrecord')\n",
    "\n",
    "# Build the model\n",
    "model = gauge_reader.build_model()\n",
    "\n",
    "# Train the model (you'll need to implement the training loop based on your specific requirements)\n",
    "# ...\n",
    "\n",
    "# Make predictions on a single image\n",
    "image = cv2.imread('.jpg')\n",
    "predictions = model(tf.convert_to_tensor([image]))\n",
    "results = gauge_reader.process_predictions(predictions, image)\n",
    "\n",
    "# Visualize results\n",
    "if results:\n",
    "    output_image = gauge_reader.visualize_results(image, results)\n",
    "    cv2.imwrite('result.jpg', output_image)\n",
    "    print(f\"Gauge Reading: {results['reading']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
